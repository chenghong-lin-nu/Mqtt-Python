{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Most of the time you won't want to train a whole convolutional network yourself. Modern ConvNets training on huge datasets like ImageNet take weeks on multiple GPUs. Instead, most people use a pretrained network either as a fixed feature extractor, or as an initial network to fine tune. In this notebook, you'll be using [VGGNet](https://arxiv.org/pdf/1409.1556.pdf) trained on the [ImageNet dataset](http://www.image-net.org/) as a feature extractor. Below is a diagram of the VGGNet architecture.\n",
    "\n",
    "<img src=\"assets/cnnarchitecture.jpg\" width=700px>\n",
    "\n",
    "VGGNet is great because it's simple and has great performance, coming in second in the ImageNet competition. The idea here is that we keep all the convolutional layers, but replace the final fully connected layers with our own classifier. This way we can use VGGNet as a feature extractor for our images then easily train a simple classifier on top of that. What we'll do is take the first fully connected layer with 4096 units, including thresholding with ReLUs. We can use those values as a code for each image, then build a classifier on top of those codes.\n",
    "\n",
    "You can read more about transfer learning from [the CS231n course notes](http://cs231n.github.io/transfer-learning/#tf).\n",
    "\n",
    "## Pretrained VGGNet\n",
    "\n",
    "We'll be using a pretrained network from https://github.com/machrisaa/tensorflow-vgg. \n",
    "\n",
    "This is a really nice implementation of VGGNet, quite easy to work with. The network has already been trained and the parameters are available from this link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter file already exists!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "\n",
    "vgg_dir = 'tensorflow_vgg/'\n",
    "# Make sure vgg exists\n",
    "if not isdir(vgg_dir):\n",
    "    raise Exception(\"VGG directory doesn't exist!\")\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(vgg_dir + \"vgg16.npy\"):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='VGG16 Parameters') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://s3.amazonaws.com/content.udacity-data.com/nd101/vgg16.npy',\n",
    "            vgg_dir + 'vgg16.npy',\n",
    "            pbar.hook)\n",
    "else:\n",
    "    print(\"Parameter file already exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flower power\n",
    "\n",
    "Here we'll be using VGGNet to classify images of flowers. To get the flower dataset, run the cell below. This dataset comes from the [TensorFlow inception tutorial](https://www.tensorflow.org/tutorials/image_retraining)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "dataset_folder_path = 'flower_photos'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('flower_photos.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Flowers Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "            'flower_photos.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(dataset_folder_path):\n",
    "    with tarfile.open('flower_photos.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet Codes\n",
    "\n",
    "Below, we'll run through all the images in our dataset and get codes for each of them. That is, we'll run the images through the VGGNet convolutional layers and record the values of the first fully connected layer. We can then write these to a file for later when we build our own classifier.\n",
    "\n",
    "Here we're using the `vgg16` module from `tensorflow_vgg`. The network takes images of size $244 \\times 224 \\times 3$ as input. Then it has 5 sets of convolutional layers. The network implemented here has this structure (copied from [the source code](https://github.com/machrisaa/tensorflow-vgg/blob/master/vgg16.py):\n",
    "\n",
    "```\n",
    "self.conv1_1 = self.conv_layer(bgr, \"conv1_1\")\n",
    "self.conv1_2 = self.conv_layer(self.conv1_1, \"conv1_2\")\n",
    "self.pool1 = self.max_pool(self.conv1_2, 'pool1')\n",
    "\n",
    "self.conv2_1 = self.conv_layer(self.pool1, \"conv2_1\")\n",
    "self.conv2_2 = self.conv_layer(self.conv2_1, \"conv2_2\")\n",
    "self.pool2 = self.max_pool(self.conv2_2, 'pool2')\n",
    "\n",
    "self.conv3_1 = self.conv_layer(self.pool2, \"conv3_1\")\n",
    "self.conv3_2 = self.conv_layer(self.conv3_1, \"conv3_2\")\n",
    "self.conv3_3 = self.conv_layer(self.conv3_2, \"conv3_3\")\n",
    "self.pool3 = self.max_pool(self.conv3_3, 'pool3')\n",
    "\n",
    "self.conv4_1 = self.conv_layer(self.pool3, \"conv4_1\")\n",
    "self.conv4_2 = self.conv_layer(self.conv4_1, \"conv4_2\")\n",
    "self.conv4_3 = self.conv_layer(self.conv4_2, \"conv4_3\")\n",
    "self.pool4 = self.max_pool(self.conv4_3, 'pool4')\n",
    "\n",
    "self.conv5_1 = self.conv_layer(self.pool4, \"conv5_1\")\n",
    "self.conv5_2 = self.conv_layer(self.conv5_1, \"conv5_2\")\n",
    "self.conv5_3 = self.conv_layer(self.conv5_2, \"conv5_3\")\n",
    "self.pool5 = self.max_pool(self.conv5_3, 'pool5')\n",
    "\n",
    "self.fc6 = self.fc_layer(self.pool5, \"fc6\")\n",
    "self.relu6 = tf.nn.relu(self.fc6)\n",
    "```\n",
    "\n",
    "So what we want are the values of the first fully connected layer, after being ReLUd (`self.relu6`). To build the network, we use\n",
    "\n",
    "```\n",
    "with tf.Session() as sess:\n",
    "    vgg = vgg16.Vgg16()\n",
    "    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    with tf.name_scope(\"content_vgg\"):\n",
    "        vgg.build(input_)\n",
    "```\n",
    "\n",
    "This creates the `vgg` object, then builds the graph with `vgg.build(input_)`. Then to get the values from the layer,\n",
    "\n",
    "```\n",
    "feed_dict = {input_: images}\n",
    "codes = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_vgg import vgg16\n",
    "from tensorflow_vgg import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'animal_photos/'\n",
    "contents = os.listdir(data_dir)\n",
    "classes = [each for each in contents if os.path.isdir(data_dir + each)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I'm running images through the VGG network in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\backend-test\\Mqtt-Python\\tensorflow_vgg\\vgg16.npy\n",
      "npy file loaded\n",
      "build model started\n",
      "build model finished: 5s\n",
      "Starting bird images\n",
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n",
      "40 images processed\n",
      "50 images processed\n",
      "60 images processed\n",
      "70 images processed\n",
      "80 images processed\n",
      "90 images processed\n",
      "100 images processed\n",
      "110 images processed\n",
      "120 images processed\n",
      "130 images processed\n",
      "140 images processed\n",
      "150 images processed\n",
      "160 images processed\n",
      "170 images processed\n",
      "180 images processed\n",
      "190 images processed\n",
      "200 images processed\n",
      "210 images processed\n",
      "220 images processed\n",
      "230 images processed\n",
      "240 images processed\n",
      "250 images processed\n",
      "260 images processed\n",
      "270 images processed\n",
      "280 images processed\n",
      "290 images processed\n",
      "300 images processed\n",
      "301 images processed\n",
      "Starting cat images\n",
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n",
      "40 images processed\n",
      "50 images processed\n",
      "60 images processed\n",
      "70 images processed\n",
      "80 images processed\n",
      "90 images processed\n",
      "100 images processed\n",
      "110 images processed\n",
      "120 images processed\n",
      "130 images processed\n",
      "140 images processed\n",
      "150 images processed\n",
      "160 images processed\n",
      "170 images processed\n",
      "180 images processed\n",
      "190 images processed\n",
      "200 images processed\n",
      "210 images processed\n",
      "220 images processed\n",
      "230 images processed\n",
      "240 images processed\n",
      "250 images processed\n",
      "260 images processed\n",
      "270 images processed\n",
      "280 images processed\n",
      "290 images processed\n",
      "300 images processed\n",
      "303 images processed\n",
      "Starting dog images\n",
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n",
      "40 images processed\n",
      "50 images processed\n",
      "60 images processed\n",
      "70 images processed\n",
      "80 images processed\n",
      "90 images processed\n",
      "100 images processed\n",
      "110 images processed\n",
      "120 images processed\n",
      "130 images processed\n",
      "140 images processed\n",
      "150 images processed\n",
      "160 images processed\n",
      "170 images processed\n",
      "180 images processed\n",
      "190 images processed\n",
      "200 images processed\n",
      "210 images processed\n",
      "220 images processed\n",
      "230 images processed\n",
      "240 images processed\n",
      "250 images processed\n",
      "260 images processed\n",
      "270 images processed\n",
      "280 images processed\n",
      "290 images processed\n",
      "300 images processed\n",
      "304 images processed\n",
      "Starting fish images\n",
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n",
      "40 images processed\n",
      "50 images processed\n",
      "60 images processed\n",
      "70 images processed\n",
      "80 images processed\n",
      "90 images processed\n",
      "100 images processed\n",
      "110 images processed\n",
      "120 images processed\n",
      "130 images processed\n",
      "140 images processed\n",
      "150 images processed\n",
      "160 images processed\n",
      "170 images processed\n",
      "180 images processed\n",
      "190 images processed\n",
      "200 images processed\n",
      "210 images processed\n",
      "220 images processed\n",
      "230 images processed\n",
      "240 images processed\n",
      "250 images processed\n",
      "260 images processed\n",
      "270 images processed\n",
      "280 images processed\n",
      "290 images processed\n",
      "296 images processed\n",
      "Starting tiger images\n",
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n",
      "40 images processed\n",
      "50 images processed\n",
      "60 images processed\n",
      "70 images processed\n",
      "80 images processed\n",
      "90 images processed\n",
      "100 images processed\n",
      "110 images processed\n",
      "120 images processed\n",
      "130 images processed\n",
      "140 images processed\n",
      "150 images processed\n",
      "160 images processed\n",
      "170 images processed\n",
      "180 images processed\n",
      "190 images processed\n",
      "200 images processed\n",
      "210 images processed\n",
      "220 images processed\n",
      "230 images processed\n",
      "240 images processed\n",
      "250 images processed\n",
      "260 images processed\n",
      "270 images processed\n",
      "280 images processed\n",
      "290 images processed\n",
      "300 images processed\n",
      "310 images processed\n",
      "320 images processed\n"
     ]
    }
   ],
   "source": [
    "# Set the batch size higher if you can fit in in your GPU memory\n",
    "batch_size = 10\n",
    "codes_list = []\n",
    "labels = []\n",
    "batch = []\n",
    "\n",
    "codes = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    vgg = vgg16.Vgg16()\n",
    "    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    with tf.name_scope(\"content_vgg\"):\n",
    "        vgg.build(input_)\n",
    "\n",
    "    for each in classes:\n",
    "        print(\"Starting {} images\".format(each))\n",
    "        class_path = data_dir + each\n",
    "        files = os.listdir(class_path)\n",
    "        for ii, file in enumerate(files, 1):\n",
    "            # Add images to the current batch\n",
    "            # utils.load_image crops the input images for us, from the center\n",
    "            img = utils.load_image(os.path.join(class_path, file))\n",
    "            batch.append(img.reshape((1, 224, 224, 3)))\n",
    "            labels.append(each)\n",
    "            \n",
    "            # Running the batch through the network to get the codes\n",
    "            if ii % batch_size == 0 or ii == len(files):\n",
    "                images = np.concatenate(batch)\n",
    "\n",
    "                feed_dict = {input_: images}\n",
    "                codes_batch = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "                \n",
    "                # Here I'm building an array of the codes\n",
    "                if codes is None:\n",
    "                    codes = codes_batch\n",
    "                else:\n",
    "                    codes = np.concatenate((codes, codes_batch))\n",
    "                \n",
    "                # Reset to start building the next batch\n",
    "                batch = []\n",
    "                print('{} images processed'.format(ii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write codes to file\n",
    "with open('codes', 'w') as f:\n",
    "    codes.tofile(f)\n",
    "    \n",
    "# write labels to file\n",
    "import csv\n",
    "with open('labels', 'w') as f:\n",
    "    writer = csv.writer(f, delimiter='\\n')\n",
    "    writer.writerow(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Classifier\n",
    "\n",
    "Now that we have codes for all the images, we can build a simple classifier on top of them. The codes behave just like normal input into a simple neural network. Below I'm going to have you do most of the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read codes and labels from file\n",
    "import csv\n",
    "\n",
    "with open('labels') as f:\n",
    "    reader = csv.reader(f, delimiter='\\n')\n",
    "    labels = np.array([each for each in reader if len(each) > 0]).squeeze()\n",
    "with open('codes') as f:\n",
    "    codes = np.fromfile(f, dtype=np.float32)\n",
    "    codes = codes.reshape((len(labels), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep\n",
    "\n",
    "As usual, now we need to one-hot encode our labels and create validation/test sets. First up, creating our labels!\n",
    "\n",
    "> **Exercise:** From scikit-learn, use [LabelBinarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) to create one-hot encoded vectors from the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(labels)\n",
    "\n",
    "labels_vecs = lb.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll want to create your training, validation, and test sets. An important thing to note here is that our labels and data aren't randomized yet. We'll want to shuffle our data so the validation and test sets contain data from all classes. Otherwise, you could end up with testing sets that are all one class. Typically, you'll also want to make sure that each smaller set has the same the distribution of classes as it is for the whole data set. The easiest way to accomplish both these goals is to use [`StratifiedShuffleSplit`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) from scikit-learn.\n",
    "\n",
    "You can create the splitter like so:\n",
    "```\n",
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "```\n",
    "Then split the data with \n",
    "```\n",
    "splitter = ss.split(x, y)\n",
    "```\n",
    "\n",
    "`ss.split` returns a generator of indices. You can pass the indices into the arrays to get the split sets. The fact that it's a generator means you either need to iterate over it, or use `next(splitter)` to get the indices. Be sure to read the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) and the [user guide](http://scikit-learn.org/stable/modules/cross_validation.html#random-permutations-cross-validation-a-k-a-shuffle-split).\n",
    "\n",
    "> **Exercise:** Use StratifiedShuffleSplit to split the codes and labels into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "\n",
    "train_idx, val_idx = next(ss.split(codes, labels_vecs))\n",
    "\n",
    "half_val_len = int(len(val_idx)/2)\n",
    "val_idx, test_idx = val_idx[:half_val_len], val_idx[half_val_len:]\n",
    "\n",
    "train_x, train_y = codes[train_idx], labels_vecs[train_idx]\n",
    "val_x, val_y = codes[val_idx], labels_vecs[val_idx]\n",
    "test_x, test_y = codes[test_idx], labels_vecs[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes (x, y): (1219, 4096) (1219, 5)\n",
      "Validation shapes (x, y): (152, 4096) (152, 5)\n",
      "Test shapes (x, y): (153, 4096) (153, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shapes (x, y):\", train_x.shape, train_y.shape)\n",
    "print(\"Validation shapes (x, y):\", val_x.shape, val_y.shape)\n",
    "print(\"Test shapes (x, y):\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did it right, you should see these sizes for the training sets:\n",
    "\n",
    "```\n",
    "Train shapes (x, y): (2936, 4096) (2936, 5)\n",
    "Validation shapes (x, y): (367, 4096) (367, 5)\n",
    "Test shapes (x, y): (367, 4096) (367, 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier layers\n",
    "\n",
    "Once you have the convolutional codes, you just need to build a classfier from some fully connected layers. You use the codes as the inputs and the image labels as targets. Otherwise the classifier is a typical neural network.\n",
    "\n",
    "> **Exercise:** With the codes and labels loaded, build the classifier. Consider the codes as your inputs, each of them are 4096D vectors. You'll want to use a hidden layer and an output layer as your classifier. Remember that the output layer needs to have one unit for each class and a softmax activation function. Use the cross entropy to calculate the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ = tf.placeholder(tf.float32, shape=[None, codes.shape[1]], name='inputs')\n",
    "labels_ = tf.placeholder(tf.int64, shape=[None, labels_vecs.shape[1]], name='labels')\n",
    "############################\n",
    "# 加下面那条是为了测试用的\n",
    "t_b1= tf.Variable(2.0,name=\"bias\")\n",
    "\n",
    "fc = tf.contrib.layers.fully_connected(inputs_, 256)\n",
    "    \n",
    "logits = tf.contrib.layers.fully_connected(fc, labels_vecs.shape[1], activation_fn=None)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels_, logits=logits)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "predicted = tf.nn.softmax(logits, name='op_predicted')\n",
    "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='op_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches!\n",
    "\n",
    "Here is just a simple way to do batches. I've written it so that it includes all the data. Sometimes you'll throw out some data at the end to make sure you have full batches. Here I just extend the last batch to include the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, n_batches=10):\n",
    "    \"\"\" Return a generator that yields batches from arrays x and y. \"\"\"\n",
    "    batch_size = len(x)//n_batches\n",
    "    \n",
    "    for ii in range(0, n_batches*batch_size, batch_size):\n",
    "        # If we're not on the last batch, grab data with size batch_size\n",
    "        if ii != (n_batches-1)*batch_size:\n",
    "            X, Y = x[ii: ii+batch_size], y[ii: ii+batch_size] \n",
    "        # On the last batch, grab the rest of the data\n",
    "        else:\n",
    "            X, Y = x[ii:], y[ii:]\n",
    "        # I love generators\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Here, we'll train the network.\n",
    "\n",
    "> **Exercise:** So far we've been providing the training code for you. Here, I'm going to give you a bit more of a challenge and have you write the code to train the network. Of course, you'll be able to see my solution if you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Iteration: 0 Training loss: 10.36278\n",
      "Epoch: 1/10 Iteration: 1 Training loss: 11.68683\n",
      "Epoch: 1/10 Iteration: 2 Training loss: 9.47348\n",
      "Epoch: 1/10 Iteration: 3 Training loss: 4.63301\n",
      "Epoch: 1/10 Iteration: 4 Training loss: 3.34701\n",
      "Epoch: 0/10 Iteration: 5 Validation Acc: 0.8750\n",
      "Epoch: 1/10 Iteration: 5 Training loss: 1.61182\n",
      "Epoch: 1/10 Iteration: 6 Training loss: 1.34595\n",
      "Epoch: 1/10 Iteration: 7 Training loss: 1.24825\n",
      "Epoch: 1/10 Iteration: 8 Training loss: 1.04111\n",
      "Epoch: 1/10 Iteration: 9 Training loss: 1.39085\n",
      "Epoch: 0/10 Iteration: 10 Validation Acc: 0.8553\n",
      "Epoch: 2/10 Iteration: 10 Training loss: 0.77126\n",
      "Epoch: 2/10 Iteration: 11 Training loss: 0.72373\n",
      "Epoch: 2/10 Iteration: 12 Training loss: 0.75915\n",
      "Epoch: 2/10 Iteration: 13 Training loss: 0.48170\n",
      "Epoch: 2/10 Iteration: 14 Training loss: 0.54844\n",
      "Epoch: 1/10 Iteration: 15 Validation Acc: 0.8750\n",
      "Epoch: 2/10 Iteration: 15 Training loss: 0.15058\n",
      "Epoch: 2/10 Iteration: 16 Training loss: 0.13131\n",
      "Epoch: 2/10 Iteration: 17 Training loss: 0.50750\n",
      "Epoch: 2/10 Iteration: 18 Training loss: 0.18003\n",
      "Epoch: 2/10 Iteration: 19 Training loss: 0.15597\n",
      "Epoch: 1/10 Iteration: 20 Validation Acc: 0.8750\n",
      "Epoch: 3/10 Iteration: 20 Training loss: 0.46009\n",
      "Epoch: 3/10 Iteration: 21 Training loss: 0.05339\n",
      "Epoch: 3/10 Iteration: 22 Training loss: 0.02058\n",
      "Epoch: 3/10 Iteration: 23 Training loss: 0.05048\n",
      "Epoch: 3/10 Iteration: 24 Training loss: 0.46139\n",
      "Epoch: 2/10 Iteration: 25 Validation Acc: 0.9211\n",
      "Epoch: 3/10 Iteration: 25 Training loss: 0.08733\n",
      "Epoch: 3/10 Iteration: 26 Training loss: 0.05040\n",
      "Epoch: 3/10 Iteration: 27 Training loss: 0.14189\n",
      "Epoch: 3/10 Iteration: 28 Training loss: 0.06379\n",
      "Epoch: 3/10 Iteration: 29 Training loss: 0.03019\n",
      "Epoch: 2/10 Iteration: 30 Validation Acc: 0.9079\n",
      "Epoch: 4/10 Iteration: 30 Training loss: 0.15157\n",
      "Epoch: 4/10 Iteration: 31 Training loss: 0.00257\n",
      "Epoch: 4/10 Iteration: 32 Training loss: 0.00392\n",
      "Epoch: 4/10 Iteration: 33 Training loss: 0.03416\n",
      "Epoch: 4/10 Iteration: 34 Training loss: 0.25260\n",
      "Epoch: 3/10 Iteration: 35 Validation Acc: 0.8684\n",
      "Epoch: 4/10 Iteration: 35 Training loss: 0.02029\n",
      "Epoch: 4/10 Iteration: 36 Training loss: 0.05906\n",
      "Epoch: 4/10 Iteration: 37 Training loss: 0.03098\n",
      "Epoch: 4/10 Iteration: 38 Training loss: 0.01597\n",
      "Epoch: 4/10 Iteration: 39 Training loss: 0.01775\n",
      "Epoch: 3/10 Iteration: 40 Validation Acc: 0.8816\n",
      "Epoch: 5/10 Iteration: 40 Training loss: 0.00680\n",
      "Epoch: 5/10 Iteration: 41 Training loss: 0.00228\n",
      "Epoch: 5/10 Iteration: 42 Training loss: 0.00428\n",
      "Epoch: 5/10 Iteration: 43 Training loss: 0.00618\n",
      "Epoch: 5/10 Iteration: 44 Training loss: 0.14433\n",
      "Epoch: 4/10 Iteration: 45 Validation Acc: 0.8882\n",
      "Epoch: 5/10 Iteration: 45 Training loss: 0.00405\n",
      "Epoch: 5/10 Iteration: 46 Training loss: 0.00276\n",
      "Epoch: 5/10 Iteration: 47 Training loss: 0.00481\n",
      "Epoch: 5/10 Iteration: 48 Training loss: 0.00380\n",
      "Epoch: 5/10 Iteration: 49 Training loss: 0.00423\n",
      "Epoch: 4/10 Iteration: 50 Validation Acc: 0.8947\n",
      "Epoch: 6/10 Iteration: 50 Training loss: 0.00585\n",
      "Epoch: 6/10 Iteration: 51 Training loss: 0.00080\n",
      "Epoch: 6/10 Iteration: 52 Training loss: 0.00402\n",
      "Epoch: 6/10 Iteration: 53 Training loss: 0.00431\n",
      "Epoch: 6/10 Iteration: 54 Training loss: 0.04149\n",
      "Epoch: 5/10 Iteration: 55 Validation Acc: 0.8947\n",
      "Epoch: 6/10 Iteration: 55 Training loss: 0.00544\n",
      "Epoch: 6/10 Iteration: 56 Training loss: 0.00189\n",
      "Epoch: 6/10 Iteration: 57 Training loss: 0.00376\n",
      "Epoch: 6/10 Iteration: 58 Training loss: 0.00384\n",
      "Epoch: 6/10 Iteration: 59 Training loss: 0.00469\n",
      "Epoch: 5/10 Iteration: 60 Validation Acc: 0.8947\n",
      "Epoch: 7/10 Iteration: 60 Training loss: 0.00273\n",
      "Epoch: 7/10 Iteration: 61 Training loss: 0.00046\n",
      "Epoch: 7/10 Iteration: 62 Training loss: 0.00370\n",
      "Epoch: 7/10 Iteration: 63 Training loss: 0.00210\n",
      "Epoch: 7/10 Iteration: 64 Training loss: 0.00307\n",
      "Epoch: 6/10 Iteration: 65 Validation Acc: 0.9013\n",
      "Epoch: 7/10 Iteration: 65 Training loss: 0.00305\n",
      "Epoch: 7/10 Iteration: 66 Training loss: 0.00121\n",
      "Epoch: 7/10 Iteration: 67 Training loss: 0.00260\n",
      "Epoch: 7/10 Iteration: 68 Training loss: 0.00306\n",
      "Epoch: 7/10 Iteration: 69 Training loss: 0.00424\n",
      "Epoch: 6/10 Iteration: 70 Validation Acc: 0.9013\n",
      "Epoch: 8/10 Iteration: 70 Training loss: 0.00154\n",
      "Epoch: 8/10 Iteration: 71 Training loss: 0.00030\n",
      "Epoch: 8/10 Iteration: 72 Training loss: 0.00190\n",
      "Epoch: 8/10 Iteration: 73 Training loss: 0.00137\n",
      "Epoch: 8/10 Iteration: 74 Training loss: 0.00177\n",
      "Epoch: 7/10 Iteration: 75 Validation Acc: 0.9013\n",
      "Epoch: 8/10 Iteration: 75 Training loss: 0.00154\n",
      "Epoch: 8/10 Iteration: 76 Training loss: 0.00080\n",
      "Epoch: 8/10 Iteration: 77 Training loss: 0.00192\n",
      "Epoch: 8/10 Iteration: 78 Training loss: 0.00161\n",
      "Epoch: 8/10 Iteration: 79 Training loss: 0.00246\n",
      "Epoch: 7/10 Iteration: 80 Validation Acc: 0.8947\n",
      "Epoch: 9/10 Iteration: 80 Training loss: 0.00104\n",
      "Epoch: 9/10 Iteration: 81 Training loss: 0.00024\n",
      "Epoch: 9/10 Iteration: 82 Training loss: 0.00103\n",
      "Epoch: 9/10 Iteration: 83 Training loss: 0.00105\n",
      "Epoch: 9/10 Iteration: 84 Training loss: 0.00118\n",
      "Epoch: 8/10 Iteration: 85 Validation Acc: 0.9013\n",
      "Epoch: 9/10 Iteration: 85 Training loss: 0.00116\n",
      "Epoch: 9/10 Iteration: 86 Training loss: 0.00061\n",
      "Epoch: 9/10 Iteration: 87 Training loss: 0.00152\n",
      "Epoch: 9/10 Iteration: 88 Training loss: 0.00119\n",
      "Epoch: 9/10 Iteration: 89 Training loss: 0.00175\n",
      "Epoch: 8/10 Iteration: 90 Validation Acc: 0.9013\n",
      "Epoch: 10/10 Iteration: 90 Training loss: 0.00082\n",
      "Epoch: 10/10 Iteration: 91 Training loss: 0.00021\n",
      "Epoch: 10/10 Iteration: 92 Training loss: 0.00076\n",
      "Epoch: 10/10 Iteration: 93 Training loss: 0.00086\n",
      "Epoch: 10/10 Iteration: 94 Training loss: 0.00093\n",
      "Epoch: 9/10 Iteration: 95 Validation Acc: 0.9013\n",
      "Epoch: 10/10 Iteration: 95 Training loss: 0.00094\n",
      "Epoch: 10/10 Iteration: 96 Training loss: 0.00052\n",
      "Epoch: 10/10 Iteration: 97 Training loss: 0.00125\n",
      "Epoch: 10/10 Iteration: 98 Training loss: 0.00101\n",
      "Epoch: 10/10 Iteration: 99 Training loss: 0.00142\n",
      "Epoch: 9/10 Iteration: 100 Validation Acc: 0.9079\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "iteration = 0\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for x, y in get_batches(train_x, train_y):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y}\n",
    "            loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                  \"Iteration: {}\".format(iteration),\n",
    "                  \"Training loss: {:.5f}\".format(loss))\n",
    "            iteration += 1\n",
    "            \n",
    "            if iteration % 5 == 0:\n",
    "                feed = {inputs_: val_x,\n",
    "                        labels_: val_y}\n",
    "                val_acc = sess.run(accuracy, feed_dict=feed)\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Validation Acc: {:.4f}\".format(val_acc))\n",
    "    saver.save(sess, \"checkpoints/flowers.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Below you see the test accuracy. You can also see the predictions returned for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如果不关闭notebook，训练完直接运行是可以执行下面的cell。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\flowers.ckpt\n",
      "Test accuracy: 0.9085\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    feed = {inputs_: test_x,\n",
    "            labels_: test_y}\n",
    "    test_acc = sess.run(accuracy, feed_dict=feed)\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read codes and labels from file\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "with open('labels') as f:\n",
    "    reader = csv.reader(f, delimiter='\\n')\n",
    "    labels = np.array([each for each in reader if len(each) > 0]).squeeze()\n",
    "with open('codes') as f:\n",
    "    codes = np.fromfile(f, dtype=np.float32)\n",
    "    codes = codes.reshape((len(labels), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(labels)\n",
    "\n",
    "labels_vecs = lb.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "\n",
    "train_idx, val_idx = next(ss.split(codes, labels_vecs))\n",
    "\n",
    "half_val_len = int(len(val_idx)/2)\n",
    "val_idx, test_idx = val_idx[:half_val_len], val_idx[half_val_len:]\n",
    "\n",
    "train_x, train_y = codes[train_idx], labels_vecs[train_idx]\n",
    "val_x, val_y = codes[val_idx], labels_vecs[val_idx]\n",
    "test_x, test_y = codes[test_idx], labels_vecs[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train shapes (x, y):\", train_x.shape, train_y.shape)\n",
    "print(\"Validation shapes (x, y):\", val_x.shape, val_y.shape)\n",
    "print(\"Test shapes (x, y):\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_x shape: '+str(test_x.shape))\n",
    "print('test_y shape: '+str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess=tf.Session()    \n",
    "#First let's load meta graph and restore weights\n",
    "saver = tf.train.import_meta_graph('checkpoints/flowers.ckpt.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('checkpoints/'))\n",
    "\n",
    "\n",
    "# Now, let's access and create placeholders variables and\n",
    "# create feed-dict to feed new data\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "inputs_ = graph.get_tensor_by_name('inputs:0')\n",
    "labels_ = graph.get_tensor_by_name('labels:0')\n",
    "\n",
    "print('inputs: '+str(inputs_))\n",
    "print('labels: '+str(labels_))\n",
    "\n",
    "\n",
    "accuracy = graph.get_tensor_by_name('op_accuracy:0')\n",
    "\n",
    "feed = {inputs_: test_x,\n",
    "        labels_: test_y}\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "test_acc = sess.run(accuracy, feed_dict=feed)\n",
    "print(\"Test accuracy: {:.4f}\".format(test_acc))\n",
    "\n",
    "\"\"\"\n",
    "w1 = graph.get_tensor_by_name(\"w1:0\")\n",
    "w2 = graph.get_tensor_by_name(\"w2:0\")\n",
    "feed_dict ={w1:13.0,w2:17.0}\n",
    "\n",
    "#Now, access the op that you want to run. \n",
    "op_to_restore = graph.get_tensor_by_name(\"op_to_restore:0\")\n",
    "\n",
    "print sess.run(op_to_restore,feed_dict)\n",
    "#This will print 60 which is calculated \n",
    "#using new values of w1 and w2 and saved value of b1. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 读取计算任务（图）\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # 载入保存的模型\n",
    "    save_dir = 'checkpoints/flowers.ckpt'\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, tf.train.latest_checkpoint('checkpoints/'))\n",
    "    \n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(sess.run('bias:0'))\n",
    "    \n",
    "    gg = tf.get_default_graph()\n",
    "    \n",
    "    inputs_ = gg.get_tensor_by_name('inputs:0')\n",
    "    labels_ = gg.get_tensor_by_name('labels:0')\n",
    "    \n",
    "    print('inputs: '+str(inputs_))\n",
    "    print('labels: '+str(labels_))\n",
    "    \n",
    "    \n",
    "    accuracy = gg.get_tensor_by_name('op_accuracy:0')\n",
    "    \n",
    "    feed = {inputs_: test_x,\n",
    "            labels_: test_y}\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    test_acc = sess.run(accuracy, feed_dict=feed)\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 读取计算任务（图）\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # 载入保存的模型\n",
    "    save_dir = 'checkpoints/flowers.ckpt'\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, tf.train.latest_checkpoint('checkpoints/'))\n",
    "    \n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(sess.run('bias:0'))\n",
    "    \n",
    "    inputs_ = sess.graph.get_tensor_by_name('inputs:0')\n",
    "    labels_ = sess.graph.get_tensor_by_name('labels:0')\n",
    "    \n",
    "    print('inputs: '+str(inputs_))\n",
    "    print('labels: '+str(labels_))\n",
    "    \n",
    "    fc = tf.contrib.layers.fully_connected(inputs_, 256)\n",
    "    \n",
    "    logits = tf.contrib.layers.fully_connected(fc, labels_vecs.shape[1], activation_fn=None)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels_, logits=logits)\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    predicted = tf.nn.softmax(logits)\n",
    "    correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    feed = {inputs_: test_x,\n",
    "            labels_: test_y}\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    test_acc = sess.run(accuracy, feed_dict=feed)\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, feel free to choose images and see how the trained classifier predicts the flowers in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = 'flower_photos/roses/10894627425_ec76bbc757_n.jpg'\n",
    "test_img = imread(test_img_path)\n",
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you don't have a vgg graph built\n",
    "with tf.Session() as sess:\n",
    "    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    vgg = vgg16.Vgg16()\n",
    "    vgg.build(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    img = utils.load_image(test_img_path)\n",
    "    img = img.reshape((1, 224, 224, 3))\n",
    "\n",
    "    feed_dict = {input_: img}\n",
    "    code = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "        \n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    feed = {inputs_: code}\n",
    "    prediction = sess.run(predicted, feed_dict=feed).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(np.arange(5), prediction)\n",
    "_ = plt.yticks(np.arange(5), lb.classes_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
